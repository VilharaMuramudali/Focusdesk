{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d62f36c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully implemented a **Hybrid Recommendation System** for FocusDesk that combines:\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Data Processing**: Loaded and preprocessed 1000 users, 500 packages, and 20,000 interaction events\n",
    "2. **Weighted Interaction Matrix**: Created a user-item matrix with event weights (Booking=1.0, Click=0.2, View=0.05)\n",
    "3. **Content-Based Filtering**: Used 16-dimensional text embeddings and cosine similarity\n",
    "4. **Collaborative Filtering**: Applied TruncatedSVD (20 components) for matrix factorization\n",
    "5. **Hybrid Model**: Combined both approaches with 60% collaborative + 40% content-based weighting\n",
    "\n",
    "### Benefits of the Hybrid Approach:\n",
    "\n",
    "- **Personalization**: Leverages user behavior patterns (collaborative filtering)\n",
    "- **Relevance**: Considers course content similarity (content-based filtering)\n",
    "- **Cold Start Mitigation**: Content-based component helps with new users\n",
    "- **Diversity**: Balances exploration (content) and exploitation (collaborative)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Fine-tune the hybrid weights (60/40 split) based on A/B testing\n",
    "- Add additional features (price, duration, difficulty level)\n",
    "- Implement online learning to update recommendations in real-time\n",
    "- Add diversity and serendipity metrics to evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b144e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "                          Collaborative Filtering Only                          \n",
      "--------------------------------------------------------------------------------\n",
      "1. Crash Mathematics Package\n",
      "   Predicted Score: 0.3570\n",
      "\n",
      "2. Master History Package\n",
      "   Predicted Score: 0.1834\n",
      "\n",
      "3. Intro English Package\n",
      "   Predicted Score: 0.1017\n",
      "\n",
      "4. Master Physics Package\n",
      "   Predicted Score: 0.0511\n",
      "\n",
      "5. Advanced Programming Package\n",
      "   Predicted Score: 0.0501\n",
      "\n",
      "\n",
      "                          Content-Based Filtering Only                          \n",
      "Based on last interacted package: Master Chemistry Package\n",
      "--------------------------------------------------------------------------------\n",
      "1. Crash English Package\n",
      "   Similarity Score: 0.6376\n",
      "\n",
      "2. Intro Physics Package\n",
      "   Similarity Score: 0.5917\n",
      "\n",
      "3. Advanced Economics Package\n",
      "   Similarity Score: 0.5766\n",
      "\n",
      "4. Intro English Package\n",
      "   Similarity Score: 0.5696\n",
      "\n",
      "5. Intro Art Package\n",
      "   Similarity Score: 0.5664\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compare with individual models\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Collaborative Filtering only\n",
    "print(f\"\\n{'Collaborative Filtering Only':^80}\")\n",
    "print(\"-\" * 80)\n",
    "collab_recs = get_collaborative_recommendations(random_user_id, n=5)\n",
    "for idx, row in collab_recs.iterrows():\n",
    "    print(f\"{idx + 1}. {row['title']}\")\n",
    "    print(f\"   Predicted Score: {row['predicted_score']:.4f}\\n\")\n",
    "\n",
    "# Content-Based Filtering only (based on last interacted package)\n",
    "user_package_scores = interaction_matrix.loc[random_user_id]\n",
    "already_interacted = user_package_scores[user_package_scores > 0].index.tolist()\n",
    "if len(already_interacted) > 0:\n",
    "    last_package = user_package_scores[user_package_scores > 0].idxmax()\n",
    "    print(f\"\\n{'Content-Based Filtering Only':^80}\")\n",
    "    print(f\"Based on last interacted package: {packages_df[packages_df['package_id'] == last_package]['title'].values[0]}\")\n",
    "    print(\"-\" * 80)\n",
    "    content_recs = get_content_recommendations(last_package, n=5)\n",
    "    for idx, row in content_recs.iterrows():\n",
    "        print(f\"{idx + 1}. {row['title']}\")\n",
    "        print(f\"   Similarity Score: {row['similarity_score']:.4f}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4675822",
   "metadata": {},
   "source": [
    "## Comparison: Individual Model Recommendations\n",
    "\n",
    "Let's compare the hybrid recommendations with individual model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2159063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "         HYBRID RECOMMENDATIONS (60% Collaborative + 40% Content-Based)         \n",
      "================================================================================\n",
      "\n",
      "Top 5 Recommended Packages:\n",
      "\n",
      "1. Crash Mathematics Package\n",
      "   Hybrid Score: 0.7436\n",
      "\n",
      "2. Master History Package\n",
      "   Hybrid Score: 0.5105\n",
      "\n",
      "3. Crash English Package\n",
      "   Hybrid Score: 0.3587\n",
      "\n",
      "4. Intro Physics Package\n",
      "   Hybrid Score: 0.3435\n",
      "\n",
      "5. Advanced Economics Package\n",
      "   Hybrid Score: 0.3428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate recommendations using the hybrid model\n",
    "print(f\"\\n{'HYBRID RECOMMENDATIONS (60% Collaborative + 40% Content-Based)':^80}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "hybrid_recommendations = recommend(random_user_id, n=5)\n",
    "\n",
    "print(\"\\nTop 5 Recommended Packages:\\n\")\n",
    "for idx, row in hybrid_recommendations.iterrows():\n",
    "    print(f\"{idx + 1}. {row['title']}\")\n",
    "    print(f\"   Hybrid Score: {row['hybrid_score']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34163550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION: Testing Hybrid Recommendation System\n",
      "================================================================================\n",
      "\n",
      "Selected User ID: user_00103\n",
      "\n",
      "                            User Interaction History                            \n",
      "--------------------------------------------------------------------------------\n",
      "Total interactions: 22\n",
      "\n",
      "Breakdown by event type:\n",
      "  - Booking: 1\n",
      "  - Click: 6\n",
      "  - Message: 3\n",
      "  - Search: 1\n",
      "  - View: 11\n",
      "\n",
      "                         Recent Interactions (Last 10)                          \n",
      "--------------------------------------------------------------------------------\n",
      "VIEW     | Crash English Package\n",
      "VIEW     | Master History Package\n",
      "VIEW     | Crash Mathematics Package\n",
      "VIEW     | Crash Design Package\n",
      "CLICK    | Crash Mathematics Package\n",
      "MESSAGE  | nan\n",
      "CLICK    | Advanced Art Package\n",
      "VIEW     | Master Programming Package\n",
      "VIEW     | Crash Chemistry Package\n",
      "MESSAGE  | nan\n",
      "\n",
      "                  Top Interacted Packages (by weighted score)                   \n",
      "--------------------------------------------------------------------------------\n",
      "Score: 1.00 | Master Chemistry Package\n",
      "Score: 0.20 | Advanced Mathematics Package\n",
      "Score: 0.20 | Intro Mathematics Package\n",
      "Score: 0.20 | Crash Mathematics Package\n",
      "Score: 0.20 | Crash Chemistry Package\n"
     ]
    }
   ],
   "source": [
    "# Select a random user for testing\n",
    "np.random.seed(42)\n",
    "random_user_id = np.random.choice(interaction_matrix.index)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"EVALUATION: Testing Hybrid Recommendation System\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSelected User ID: {random_user_id}\")\n",
    "\n",
    "# Get user's interaction history\n",
    "user_interactions = events_df[events_df['user_id'] == random_user_id].copy()\n",
    "user_interactions = user_interactions.merge(\n",
    "    packages_df[['package_id', 'title']], \n",
    "    on='package_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\n{'User Interaction History':^80}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total interactions: {len(user_interactions)}\")\n",
    "print(f\"\\nBreakdown by event type:\")\n",
    "event_summary = user_interactions.groupby('event_type').size()\n",
    "for event_type, count in event_summary.items():\n",
    "    print(f\"  - {event_type.capitalize()}: {count}\")\n",
    "\n",
    "print(f\"\\n{'Recent Interactions (Last 10)':^80}\")\n",
    "print(\"-\" * 80)\n",
    "recent_interactions = user_interactions.sort_index(ascending=False).head(10)\n",
    "for idx, row in recent_interactions.iterrows():\n",
    "    print(f\"{row['event_type'].upper():8} | {row['title']}\")\n",
    "\n",
    "print(f\"\\n{'Top Interacted Packages (by weighted score)':^80}\")\n",
    "print(\"-\" * 80)\n",
    "user_package_scores = interaction_matrix.loc[random_user_id]\n",
    "top_packages = user_package_scores[user_package_scores > 0].sort_values(ascending=False).head(5)\n",
    "for package_id, score in top_packages.items():\n",
    "    package_title = packages_df[packages_df['package_id'] == package_id]['title'].values[0]\n",
    "    print(f\"Score: {score:.2f} | {package_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356afaea",
   "metadata": {},
   "source": [
    "## 9. Evaluation and Testing\n",
    "\n",
    "Test the hybrid recommendation system with a random user, showing their interaction history and personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c327a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Hybrid Recommendation System for User ID: user_00011\n",
      "\n",
      "Top 5 Hybrid Recommendations:\n",
      "  package_id                       title  hybrid_score\n",
      "0   pkg_0032    Advanced Physics Package      0.782209\n",
      "1   pkg_0330     Crash Chemistry Package      0.719212\n",
      "2   pkg_0030      Master Biology Package      0.592355\n",
      "3   pkg_0468  Master Mathematics Package      0.588874\n",
      "4   pkg_0240   Crash Mathematics Package      0.583956\n"
     ]
    }
   ],
   "source": [
    "def recommend(user_id, n=5, collaborative_weight=0.6, content_weight=0.4):\n",
    "    \"\"\"\n",
    "    Hybrid recommendation system combining collaborative and content-based filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_id : int\n",
    "        The user ID to get recommendations for\n",
    "    n : int\n",
    "        Number of recommendations to return (default: 5)\n",
    "    collaborative_weight : float\n",
    "        Weight for collaborative filtering (default: 0.6)\n",
    "    content_weight : float\n",
    "        Weight for content-based filtering (default: 0.4)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with top N recommended package titles and hybrid scores\n",
    "    \"\"\"\n",
    "    if user_id not in predicted_scores_df.index:\n",
    "        print(f\"User ID {user_id} not found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Step 1: Get collaborative filtering scores\n",
    "    user_predictions = predicted_scores_df.loc[user_id]\n",
    "    \n",
    "    # Get packages the user has already interacted with\n",
    "    interacted_packages = interaction_matrix.loc[user_id]\n",
    "    already_interacted = interacted_packages[interacted_packages > 0].index.tolist()\n",
    "    \n",
    "    # Step 2: Get the user's last interacted package for content-based filtering\n",
    "    if len(already_interacted) > 0:\n",
    "        # Find the most recent or highest-weighted interaction\n",
    "        last_package_id = interacted_packages[interacted_packages > 0].idxmax()\n",
    "        \n",
    "        # Get content-based scores\n",
    "        if last_package_id in content_similarity_df.index:\n",
    "            content_scores = content_similarity_df[last_package_id]\n",
    "        else:\n",
    "            # Fallback: use zero content scores\n",
    "            content_scores = pd.Series(0, index=content_similarity_df.index)\n",
    "    else:\n",
    "        # If user has no interactions, use only collaborative filtering\n",
    "        content_scores = pd.Series(0, index=user_predictions.index)\n",
    "        content_weight = 0\n",
    "        collaborative_weight = 1.0\n",
    "    \n",
    "    # Step 3: Normalize scores to 0-1 range for fair combination\n",
    "    # Normalize collaborative scores\n",
    "    collab_min, collab_max = user_predictions.min(), user_predictions.max()\n",
    "    if collab_max > collab_min:\n",
    "        collab_normalized = (user_predictions - collab_min) / (collab_max - collab_min)\n",
    "    else:\n",
    "        collab_normalized = user_predictions\n",
    "    \n",
    "    # Normalize content scores\n",
    "    content_min, content_max = content_scores.min(), content_scores.max()\n",
    "    if content_max > content_min:\n",
    "        content_normalized = (content_scores - content_min) / (content_max - content_min)\n",
    "    else:\n",
    "        content_normalized = content_scores\n",
    "    \n",
    "    # Step 4: Combine scores with weighted average\n",
    "    # Align indices\n",
    "    common_packages = collab_normalized.index.intersection(content_normalized.index)\n",
    "    hybrid_scores = (\n",
    "        collaborative_weight * collab_normalized[common_packages] +\n",
    "        content_weight * content_normalized[common_packages]\n",
    "    )\n",
    "    \n",
    "    # Step 5: Filter out already interacted packages\n",
    "    uninteracted_scores = hybrid_scores[~hybrid_scores.index.isin(already_interacted)]\n",
    "    \n",
    "    # Step 6: Get top N recommendations\n",
    "    top_recommendations = uninteracted_scores.sort_values(ascending=False).head(n)\n",
    "    \n",
    "    # Create result DataFrame\n",
    "    result = pd.DataFrame({\n",
    "        'package_id': top_recommendations.index,\n",
    "        'hybrid_score': top_recommendations.values\n",
    "    })\n",
    "    \n",
    "    # Add package titles\n",
    "    result = result.merge(\n",
    "        packages_df[['package_id', 'title']], \n",
    "        on='package_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return result[['package_id', 'title', 'hybrid_score']]\n",
    "\n",
    "# Test the hybrid recommendation function\n",
    "test_user_id = interaction_matrix.index[10]\n",
    "print(f\"Testing Hybrid Recommendation System for User ID: {test_user_id}\")\n",
    "print(\"\\nTop 5 Hybrid Recommendations:\")\n",
    "print(recommend(test_user_id, n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a01e86",
   "metadata": {},
   "source": [
    "## 8. Hybrid Recommendation Engine\n",
    "\n",
    "**Goal**: Combine collaborative and content-based filtering for final recommendations.\n",
    "\n",
    "**Hybrid Formula:**\n",
    "```\n",
    "Hybrid Score = (0.6 × Collaborative Score) + (0.4 × Content Score)\n",
    "```\n",
    "\n",
    "The hybrid approach:\n",
    "1. Gets collaborative scores (user preferences)\n",
    "2. Gets content scores (based on user's last interacted package)\n",
    "3. Normalizes both to 0-1 range\n",
    "4. Combines with weighted average\n",
    "5. Returns top N unvisited packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cdbea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing collaborative filtering for User ID: user_00001\n",
      "\n",
      "Top 5 Recommended Packages:\n",
      "  package_id                       title  predicted_score\n",
      "0   pkg_0316        Crash Design Package         0.010364\n",
      "1   pkg_0324  Master Programming Package         0.010156\n",
      "2   pkg_0327    Advanced History Package         0.006363\n",
      "3   pkg_0108  Advanced Economics Package         0.006361\n",
      "4   pkg_0393       Crash English Package         0.006043\n"
     ]
    }
   ],
   "source": [
    "def get_collaborative_recommendations(user_id, n=5):\n",
    "    \"\"\"\n",
    "    Get top N package recommendations for a user based on collaborative filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_id : str\n",
    "        The user ID to get recommendations for\n",
    "    n : int\n",
    "        Number of recommendations to return (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with package_id, title, subject, and predicted_score\n",
    "    \"\"\"\n",
    "    if user_id not in predicted_scores_df.index:\n",
    "        print(f\"❌ Error: User ID '{user_id}' not found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get predicted scores for the user\n",
    "    user_predictions = predicted_scores_df.loc[user_id]\n",
    "    \n",
    "    # Get packages the user has already interacted with\n",
    "    interacted_packages = interaction_matrix.loc[user_id]\n",
    "    already_interacted = interacted_packages[interacted_packages > 0].index.tolist()\n",
    "    \n",
    "    # Filter out already interacted packages\n",
    "    uninteracted_predictions = user_predictions[~user_predictions.index.isin(already_interacted)]\n",
    "    \n",
    "    # Get top N recommendations\n",
    "    top_recommendations = uninteracted_predictions.sort_values(ascending=False).head(n)\n",
    "    \n",
    "    # Create result DataFrame\n",
    "    result = pd.DataFrame({\n",
    "        'package_id': top_recommendations.index,\n",
    "        'predicted_score': top_recommendations.values\n",
    "    })\n",
    "    \n",
    "    # Add package details\n",
    "    result = result.merge(\n",
    "        packages_df[['package_id', 'title', 'subject']], \n",
    "        on='package_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return result[['package_id', 'title', 'subject', 'predicted_score']]\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_user_id = interaction_matrix.index[5]\n",
    "\n",
    "print(f\"Testing Collaborative Filtering Recommendations\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"User ID: {test_user_id}\")\n",
    "print(f\"\\nTop 5 Recommended Packages:\")\n",
    "print(\"=\"*70)\n",
    "recommendations = get_collaborative_recommendations(test_user_id, n=5)\n",
    "for idx, row in recommendations.iterrows():\n",
    "    print(f\"{idx+1}. {row['title']}\")\n",
    "    print(f\"   Subject: {row['subject']} | Predicted Score: {row['predicted_score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03058781",
   "metadata": {},
   "source": [
    "## 7. Collaborative Filtering Recommendation Function\n",
    "\n",
    "**Goal**: Implement a function to get recommendations based on collaborative filtering.\n",
    "\n",
    "This function predicts scores for packages the user hasn't interacted with and returns the top N recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5bfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD Model trained successfully!\n",
      "Number of components: 20\n",
      "User factors shape: (1000, 20)\n",
      "Package factors shape: (500, 20)\n",
      "Explained variance ratio: 0.1596\n",
      "\n",
      "Predicted scores matrix shape: (1000, 500)\n"
     ]
    }
   ],
   "source": [
    "# Apply TruncatedSVD (Matrix Factorization)\n",
    "n_components = 20\n",
    "random_state = 42\n",
    "\n",
    "print(f\"Training TruncatedSVD with {n_components} components...\")\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "user_factors = svd.fit_transform(interaction_matrix)\n",
    "package_factors = svd.components_.T\n",
    "\n",
    "print(f\"✓ SVD Model trained successfully!\")\n",
    "print(f\"\\nModel Details:\")\n",
    "print(f\"  - Number of components: {n_components}\")\n",
    "print(f\"  - User factors shape: {user_factors.shape}\")\n",
    "print(f\"  - Package factors shape: {package_factors.shape}\")\n",
    "print(f\"  - Explained variance ratio: {svd.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Reconstruct the prediction matrix\n",
    "predicted_scores = np.dot(user_factors, package_factors.T)\n",
    "predicted_scores_df = pd.DataFrame(\n",
    "    predicted_scores,\n",
    "    index=interaction_matrix.index,\n",
    "    columns=interaction_matrix.columns\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Prediction matrix reconstructed!\")\n",
    "print(f\"  - Shape: {predicted_scores_df.shape}\")\n",
    "print(f\"  - Data type: {predicted_scores_df.dtypes[0]}\")\n",
    "\n",
    "print(f\"\\nSample Predicted Scores (First 5×5):\")\n",
    "print(\"=\"*70)\n",
    "print(predicted_scores_df.iloc[:5, :5].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90ddbf",
   "metadata": {},
   "source": [
    "## 6. Collaborative Filtering - Matrix Factorization\n",
    "\n",
    "**Goal**: Apply TruncatedSVD to learn latent user and package features.\n",
    "\n",
    "TruncatedSVD decomposes the interaction matrix into:\n",
    "- **User factors**: Latent user preferences\n",
    "- **Package factors**: Latent package characteristics\n",
    "\n",
    "We'll use 20 components to capture the main patterns in user-package interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120fa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing content-based recommendations for Package ID: pkg_0001\n",
      "Package: Crash Design Package\n",
      "\n",
      "Top 5 Similar Packages:\n",
      "  package_id                     title  similarity_score\n",
      "0   pkg_0111      Crash Design Package          0.750600\n",
      "1   pkg_0314      Intro Design Package          0.640719\n",
      "2   pkg_0369     Master Design Package          0.618573\n",
      "3   pkg_0329     Crash Physics Package          0.617461\n",
      "4   pkg_0300  Advanced Physics Package          0.599338\n"
     ]
    }
   ],
   "source": [
    "def get_content_recommendations(package_id, n=5):\n",
    "    \"\"\"\n",
    "    Get top N similar packages based on content similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    package_id : str\n",
    "        The package ID to find similar packages for\n",
    "    n : int\n",
    "        Number of recommendations to return (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with package_id, title, and similarity_score\n",
    "    \"\"\"\n",
    "    if package_id not in content_similarity_df.index:\n",
    "        print(f\"❌ Error: Package ID '{package_id}' not found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get similarity scores for the given package\n",
    "    similarities = content_similarity_df[package_id].sort_values(ascending=False)\n",
    "    \n",
    "    # Exclude the package itself and get top N\n",
    "    similar_packages = similarities[similarities.index != package_id].head(n)\n",
    "    \n",
    "    # Create result DataFrame\n",
    "    result = pd.DataFrame({\n",
    "        'package_id': similar_packages.index,\n",
    "        'similarity_score': similar_packages.values\n",
    "    })\n",
    "    \n",
    "    # Add package titles\n",
    "    result = result.merge(\n",
    "        packages_df[['package_id', 'title', 'subject']], \n",
    "        on='package_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return result[['package_id', 'title', 'subject', 'similarity_score']]\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_package_id = packages_df['package_id'].iloc[0]\n",
    "test_package_title = packages_df[packages_df['package_id'] == test_package_id]['title'].values[0]\n",
    "\n",
    "print(f\"Testing Content-Based Recommendations\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Input Package: {test_package_id} - '{test_package_title}'\")\n",
    "print(f\"\\nTop 5 Similar Packages:\")\n",
    "print(\"=\"*70)\n",
    "recommendations = get_content_recommendations(test_package_id, n=5)\n",
    "for idx, row in recommendations.iterrows():\n",
    "    print(f\"{idx+1}. {row['title']}\")\n",
    "    print(f\"   Subject: {row['subject']} | Similarity: {row['similarity_score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff5214",
   "metadata": {},
   "source": [
    "## 5. Content-Based Recommendation Function\n",
    "\n",
    "**Goal**: Implement a function to get similar packages based on content.\n",
    "\n",
    "This function takes a `package_id` and returns the top N most similar packages using the cosine similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c336d8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (500, 16)\n",
      "\n",
      "Content Similarity Matrix Shape: (500, 500)\n",
      "\n",
      "Sample similarities (first 5x5):\n",
      "package_id  pkg_0001  pkg_0002  pkg_0003  pkg_0004  pkg_0005\n",
      "package_id                                                  \n",
      "pkg_0001    1.000000 -0.075379 -0.390542 -0.101092 -0.255605\n",
      "pkg_0002   -0.075379  1.000000 -0.099113 -0.159718  0.051001\n",
      "pkg_0003   -0.390542 -0.099113  1.000000  0.106459 -0.313470\n",
      "pkg_0004   -0.101092 -0.159718  0.106459  1.000000  0.018445\n",
      "pkg_0005   -0.255605  0.051001 -0.313470  0.018445  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Create embedding matrix from parsed vectors\n",
    "embedding_matrix = np.vstack(packages_df['embedding_vector'].values)\n",
    "\n",
    "print(f\"✓ Embedding matrix created!\")\n",
    "print(f\"  - Shape: {embedding_matrix.shape}\")\n",
    "print(f\"  - Data type: {embedding_matrix.dtype}\")\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "content_similarity = cosine_similarity(embedding_matrix)\n",
    "\n",
    "print(f\"\\n✓ Cosine similarity computed!\")\n",
    "print(f\"  - Similarity matrix shape: {content_similarity.shape}\")\n",
    "\n",
    "# Create DataFrame for easier lookup\n",
    "content_similarity_df = pd.DataFrame(\n",
    "    content_similarity,\n",
    "    index=packages_df['package_id'],\n",
    "    columns=packages_df['package_id']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Content Similarity Matrix ready!\")\n",
    "print(f\"\\nSimilarity Statistics:\")\n",
    "print(f\"  - Min similarity: {content_similarity.min():.4f}\")\n",
    "print(f\"  - Max similarity: {content_similarity.max():.4f}\")\n",
    "print(f\"  - Mean similarity: {content_similarity.mean():.4f}\")\n",
    "\n",
    "print(f\"\\nSample Similarity Matrix (First 5×5):\")\n",
    "print(\"=\"*70)\n",
    "print(content_similarity_df.iloc[:5, :5].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06457838",
   "metadata": {},
   "source": [
    "## 4. Content-Based Model - Cosine Similarity\n",
    "\n",
    "**Goal**: Calculate similarity between packages based on their text embeddings.\n",
    "\n",
    "We'll compute a **Cosine Similarity Matrix** where each cell (i, j) represents how similar package i is to package j based on their 16-dimensional text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef8720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction Matrix Shape: (1000, 500)\n",
      "Users: 1000, Packages: 500\n",
      "Sparsity: 97.24%\n",
      "\n",
      "Interaction Matrix (first 5x5):\n",
      "package_id  pkg_0001  pkg_0002  pkg_0003  pkg_0004  pkg_0005\n",
      "user_id                                                     \n",
      "user_00001       0.0       0.0       0.0       0.0       0.0\n",
      "user_00002       0.0       0.0       0.0       0.0       0.0\n",
      "user_00003       0.0       0.0       0.0       0.0       0.0\n",
      "user_00004       0.0       0.2       0.0       0.0       0.0\n",
      "user_00005       0.0       0.0       0.0       0.0       0.0\n"
     ]
    }
   ],
   "source": [
    "# Create User-Item Interaction Matrix\n",
    "interaction_matrix = interaction_scores.pivot(\n",
    "    index='user_id',\n",
    "    columns='package_id',\n",
    "    values='score'\n",
    ").fillna(0)  # Fill missing values with 0\n",
    "\n",
    "print(f\"✓ User-Item Interaction Matrix created!\")\n",
    "print(f\"\\nMatrix Dimensions:\")\n",
    "print(f\"  - Users (rows): {interaction_matrix.shape[0]:,}\")\n",
    "print(f\"  - Packages (columns): {interaction_matrix.shape[1]:,}\")\n",
    "print(f\"  - Total cells: {interaction_matrix.shape[0] * interaction_matrix.shape[1]:,}\")\n",
    "\n",
    "# Calculate sparsity\n",
    "total_cells = interaction_matrix.shape[0] * interaction_matrix.shape[1]\n",
    "zero_cells = (interaction_matrix == 0).sum().sum()\n",
    "sparsity = (zero_cells / total_cells) * 100\n",
    "\n",
    "print(f\"\\nMatrix Sparsity:\")\n",
    "print(f\"  - Zero cells: {zero_cells:,}\")\n",
    "print(f\"  - Non-zero cells: {total_cells - zero_cells:,}\")\n",
    "print(f\"  - Sparsity: {sparsity:.2f}%\")\n",
    "\n",
    "print(f\"\\nSample Matrix (First 5 users × 5 packages):\")\n",
    "print(\"=\"*70)\n",
    "print(interaction_matrix.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4d5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique user-package interactions: 14954\n",
      "\n",
      "Sample interaction scores:\n",
      "      user_id package_id  score\n",
      "0  user_00001   pkg_0050   0.20\n",
      "1  user_00001   pkg_0053   0.05\n",
      "2  user_00001   pkg_0135   0.05\n",
      "3  user_00001   pkg_0145   0.05\n",
      "4  user_00001   pkg_0214   0.05\n",
      "5  user_00001   pkg_0232   0.05\n",
      "6  user_00001   pkg_0242   0.05\n",
      "7  user_00001   pkg_0337   0.05\n",
      "8  user_00001   pkg_0344   0.05\n",
      "9  user_00001   pkg_0410   0.05\n"
     ]
    }
   ],
   "source": [
    "# Aggregate weighted scores by user-package pairs\n",
    "interaction_scores = events_df.groupby(['user_id', 'package_id'])['weight'].sum().reset_index()\n",
    "interaction_scores.columns = ['user_id', 'package_id', 'score']\n",
    "\n",
    "print(f\"✓ Interaction scores aggregated!\")\n",
    "print(f\"\\nInteraction Summary:\")\n",
    "print(f\"  - Total unique user-package pairs: {len(interaction_scores):,}\")\n",
    "print(f\"  - Unique users: {interaction_scores['user_id'].nunique():,}\")\n",
    "print(f\"  - Unique packages: {interaction_scores['package_id'].nunique():,}\")\n",
    "print(f\"\\nScore Statistics:\")\n",
    "print(f\"  - Min score: {interaction_scores['score'].min():.4f}\")\n",
    "print(f\"  - Max score: {interaction_scores['score'].max():.4f}\")\n",
    "print(f\"  - Mean score: {interaction_scores['score'].mean():.4f}\")\n",
    "print(f\"\\nSample Interaction Scores:\")\n",
    "print(\"=\"*70)\n",
    "print(interaction_scores.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceed085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Found 6027 events with unmapped types\n",
      "Unique unmapped types: ['start_booking' 'search' 'rating' 'message']\n",
      "\n",
      "Event type distribution:\n",
      "event_type\n",
      "view             9983\n",
      "click            3611\n",
      "search           2444\n",
      "message          2392\n",
      "start_booking     608\n",
      "rating            583\n",
      "booking           379\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define event type weights\n",
    "event_weights = {\n",
    "    'booking': 1.0,      # Strongest signal - actual purchase\n",
    "    'start_booking': 1.0, # Also treat booking initiation as strong signal\n",
    "    'click': 0.2,        # Medium signal - clicked for details\n",
    "    'view': 0.05         # Weak signal - just browsed\n",
    "}\n",
    "\n",
    "# Apply weights to event types\n",
    "events_df['weight'] = events_df['event_type'].map(event_weights)\n",
    "\n",
    "# Check for any unmapped event types\n",
    "unmapped_events = events_df[events_df['weight'].isna()]\n",
    "if len(unmapped_events) > 0:\n",
    "    print(f\"⚠ Warning: Found {len(unmapped_events)} unmapped events\")\n",
    "    print(f\"  Unique unmapped types: {unmapped_events['event_type'].unique()}\")\n",
    "    # Fill unmapped with minimal weight\n",
    "    events_df['weight'].fillna(0.05, inplace=True)\n",
    "else:\n",
    "    print(\"✓ All event types mapped successfully!\")\n",
    "\n",
    "print(f\"\\nEvent Type Distribution:\")\n",
    "print(\"=\"*50)\n",
    "event_counts = events_df['event_type'].value_counts()\n",
    "for event_type, count in event_counts.items():\n",
    "    weight = event_weights.get(event_type, 0.05)\n",
    "    print(f\"  {event_type:15s}: {count:6,} events (weight: {weight})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da42c07b",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering - Interaction Matrix\n",
    "\n",
    "**Goal**: Create a weighted User-Item Interaction Matrix based on event types.\n",
    "\n",
    "**Event Weights:**\n",
    "- **Booking** = 1.0 (strong positive signal)\n",
    "- **Click** = 0.2 (medium interest)\n",
    "- **View** = 0.05 (weak interest)\n",
    "\n",
    "We'll aggregate these weighted scores to create a matrix where rows represent users, columns represent packages, and values represent interaction strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d12efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding parsing complete!\n",
      "Embedding dimension: 16\n",
      "Sample embedding vector:\n",
      "[ 0.3342 -0.1553 -1.9078 -0.8604 -0.4136  1.8877  0.5566 -1.3355  0.486\n",
      " -1.5473  1.0827 -0.4711 -0.0936  1.3258 -1.2872 -1.3971]\n"
     ]
    }
   ],
   "source": [
    "# Parse text_embedding column from JSON strings to numpy arrays\n",
    "def parse_embedding(embedding_str):\n",
    "    \"\"\"Convert JSON string to numpy array\"\"\"\n",
    "    try:\n",
    "        return np.array(json.loads(embedding_str))\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to parse embedding: {e}\")\n",
    "        return np.zeros(16)  # Return zero vector if parsing fails\n",
    "\n",
    "packages_df['embedding_vector'] = packages_df['text_embedding'].apply(parse_embedding)\n",
    "\n",
    "# Verify the parsing\n",
    "sample_embedding = packages_df['embedding_vector'].iloc[0]\n",
    "print(\"✓ Text embedding parsing complete!\")\n",
    "print(f\"\\nEmbedding Details:\")\n",
    "print(f\"  - Dimension: {len(sample_embedding)}\")\n",
    "print(f\"  - Data type: {type(sample_embedding)}\")\n",
    "print(f\"  - Sample vector (Package 1):\\n    {sample_embedding}\")\n",
    "print(f\"  - Vector shape: {sample_embedding.shape}\")\n",
    "\n",
    "# Verify all embeddings were parsed correctly\n",
    "valid_embeddings = packages_df['embedding_vector'].apply(lambda x: len(x) == 16).sum()\n",
    "print(f\"\\n✓ Successfully parsed {valid_embeddings}/{len(packages_df)} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e8dca",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "**Goal**: Parse the `text_embedding` column from JSON strings into numpy arrays.\n",
    "\n",
    "The `packages.csv` file contains a `text_embedding` column stored as JSON strings representing 16-dimensional vectors. We need to convert these strings into numpy arrays for cosine similarity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf3f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Users Data:\n",
      "       userId  age  country language educationLevel  isEducator  \\\n",
      "0  user_00001   31   Canada       en     HighSchool           0   \n",
      "1  user_00002   26       UK       en     HighSchool           0   \n",
      "2  user_00003   40  Germany       ta     HighSchool           0   \n",
      "\n",
      "                    createdAt learningPreferences_subjects learningStyle  \\\n",
      "0  2025-01-21T05:33:34.719956                    Chemistry        Visual   \n",
      "1  2025-01-27T05:33:34.719956  History|Mathematics|Biology         Mixed   \n",
      "2  2025-03-06T05:33:34.719956   Design|Programming|English      Auditory   \n",
      "\n",
      "  academicLevel timePreferences  aiFeatures_interactionCount  \\\n",
      "0      Advanced         Morning                           23   \n",
      "1  Intermediate       Afternoon                           15   \n",
      "2      Beginner         Evening                           25   \n",
      "\n",
      "        aiFeatures_lastActive  \n",
      "0  2025-09-08T05:33:34.720972  \n",
      "1  2025-09-26T05:33:34.721034  \n",
      "2  2025-11-09T05:33:34.721092  \n",
      "\n",
      "Sample Packages Data:\n",
      "  package_id educatorId                         title  \\\n",
      "0   pkg_0001  educ_0191          Crash Design Package   \n",
      "1   pkg_0002  educ_0110         Intro History Package   \n",
      "2   pkg_0003  educ_0169  Advanced Programming Package   \n",
      "\n",
      "                          description             keywords      subject  \\\n",
      "0  A project-based package in Design.              English    Chemistry   \n",
      "1      A hands-on package in History.          Physics|Art    Chemistry   \n",
      "2   A concise package in Programming.  Physics|Mathematics  Programming   \n",
      "\n",
      "  academicLevel languages   price currency  ...             availability  \\\n",
      "0      Beginner     en|ta    5.00      USD  ...  Thu|Sun|Sat|Fri|Wed|Tue   \n",
      "1      Beginner     en|fr  245.76      USD  ...      Wed|Thu|Mon|Fri|Sat   \n",
      "2      Advanced        en  300.00      USD  ...                  Fri|Mon   \n",
      "\n",
      "   avgRating totalReviews educatorIsPro  educatorResponseTime  \\\n",
      "0       4.45            7             0                    20   \n",
      "1       3.55            6             0                    15   \n",
      "2       5.00            5             0                     2   \n",
      "\n",
      "  educatorAvgRating                           educatorSubjects  \\\n",
      "0              5.00                                    History   \n",
      "1              3.78  Chemistry|Programming|Biology|Mathematics   \n",
      "2              4.35                          Economics|Biology   \n",
      "\n",
      "   educatorLocation                                     text_embedding  \\\n",
      "0                UK  [0.3342, -0.1553, -1.9078, -0.8604, -0.4136, 1...   \n",
      "1             India  [-0.4512, 0.5517, 1.2003, -0.4632, -0.4114, 1....   \n",
      "2         Sri Lanka  [0.6102, 0.157, -0.5865, 0.2242, 0.7146, -2.04...   \n",
      "\n",
      "                                    embedding_vector  \n",
      "0  [0.3342, -0.1553, -1.9078, -0.8604, -0.4136, 1...  \n",
      "1  [-0.4512, 0.5517, 1.2003, -0.4632, -0.4114, 1....  \n",
      "2  [0.6102, 0.157, -0.5865, 0.2242, 0.7146, -2.04...  \n",
      "\n",
      "[3 rows x 26 columns]\n",
      "\n",
      "Sample Events Data:\n",
      "      user_id     event_type package_id  sessionTime  paid  sessionDuration  \\\n",
      "0  user_00062          click   pkg_0413         33.1   0.0              NaN   \n",
      "1  user_00513           view   pkg_0121         22.2   0.0              NaN   \n",
      "2  user_00279  start_booking   pkg_0450         45.6   0.0             90.0   \n",
      "\n",
      "                    timestamp query_text filters  resultsCount  rating  \\\n",
      "0  2025-07-30T08:34:18.903741        NaN     NaN           NaN     NaN   \n",
      "1  2025-11-22T00:30:24.413398        NaN     NaN           NaN     NaN   \n",
      "2  2025-07-20T21:45:48.022629        NaN     NaN           NaN     NaN   \n",
      "\n",
      "  review_text educatorId  messageCount lastMessageAt  weight  \n",
      "0         NaN        NaN           NaN           NaN    0.20  \n",
      "1         NaN        NaN           NaN           NaN    0.05  \n",
      "2         NaN        NaN           NaN           NaN     NaN  \n"
     ]
    }
   ],
   "source": [
    "# Standardize column names and display sample data\n",
    "events_df = events_df.rename(columns={\n",
    "    'userId': 'user_id',\n",
    "    'eventType': 'event_type',\n",
    "    'packageId': 'package_id'\n",
    "})\n",
    "\n",
    "packages_df = packages_df.rename(columns={\n",
    "    'packageId': 'package_id'\n",
    "})\n",
    "\n",
    "print(\"Sample Data Preview:\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"USERS (First 3 rows)\")\n",
    "print(\"=\"*80)\n",
    "print(users_df.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PACKAGES (First 3 rows)\")\n",
    "print(\"=\"*80)\n",
    "print(packages_df[['package_id', 'title', 'subject', 'price', 'text_embedding']].head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVENTS (First 5 rows)\")\n",
    "print(\"=\"*80)\n",
    "print(events_df[['user_id', 'event_type', 'package_id']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096dae6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "\n",
      "Users: 1000 rows\n",
      "Packages: 500 rows\n",
      "Events: 20000 rows\n",
      "Ranking Examples: 20000 rows\n"
     ]
    }
   ],
   "source": [
    "# Load all CSV files\n",
    "data_path = 'ML/recommender_dataset/'  # Path to the data files\n",
    "\n",
    "users_df = pd.read_csv(data_path + 'users.csv')\n",
    "packages_df = pd.read_csv(data_path + 'packages.csv')\n",
    "events_df = pd.read_csv(data_path + 'events.csv')\n",
    "ranking_examples_df = pd.read_csv(data_path + 'ranking_examples.csv')\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"\\nUsers: {len(users_df)} rows\")\n",
    "print(f\"Packages: {len(packages_df)} rows\")\n",
    "print(f\"Events: {len(events_df)} rows\")\n",
    "print(f\"Ranking Examples: {len(ranking_examples_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb7a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names standardized to snake_case format\n",
      "Events columns: ['user_id', 'event_type', 'package_id', 'sessionTime', 'paid']\n",
      "Packages columns: ['package_id', 'educatorId', 'title', 'description', 'keywords']\n"
     ]
    }
   ],
   "source": [
    "# Load CSV files from the ML/recommender_dataset directory\n",
    "data_path = 'ML/recommender_dataset/'\n",
    "\n",
    "users_df = pd.read_csv(data_path + 'users.csv')\n",
    "packages_df = pd.read_csv(data_path + 'packages.csv')\n",
    "events_df = pd.read_csv(data_path + 'events.csv')\n",
    "ranking_examples_df = pd.read_csv(data_path + 'ranking_examples.csv')\n",
    "\n",
    "print(\"✓ Data loaded successfully!\")\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  - Users: {len(users_df):,} rows\")\n",
    "print(f\"  - Packages: {len(packages_df):,} rows\")\n",
    "print(f\"  - Events: {len(events_df):,} rows\")\n",
    "print(f\"  - Ranking Examples: {len(ranking_examples_df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6492b3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")\n",
    "print(f\"  - pandas: {pd.__version__}\")\n",
    "print(f\"  - numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f2a04",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading\n",
    "\n",
    "Import necessary libraries and load the synthetic data from CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb08f9",
   "metadata": {},
   "source": [
    "# FocusDesk Hybrid Recommendation System\n",
    "\n",
    "This notebook implements a **Hybrid Recommendation System** for the FocusDesk educational platform, combining:\n",
    "\n",
    "1. **Content-Based Filtering**: Uses text embeddings and cosine similarity\n",
    "2. **Collaborative Filtering**: Uses matrix factorization (TruncatedSVD) on user interactions\n",
    "3. **Hybrid Approach**: Weighted combination (60% Collaborative + 40% Content-Based)\n",
    "\n",
    "---\n",
    "\n",
    "**Data Sources:**\n",
    "- `users.csv` - 1000 user profiles\n",
    "- `packages.csv` - 500 educational packages with 16D text embeddings\n",
    "- `events.csv` - 20,000 user interaction events (view, click, booking)\n",
    "- `ranking_examples.csv` - Pre-processed training data (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2da818",
   "metadata": {},
   "source": [
    "## Model Accuracy & Performance Evaluation\n",
    "\n",
    "Evaluate the recommendation system using standard metrics like Precision@K, Recall@K, and NDCG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee3dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics for Recommendation System\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "def precision_at_k(actual, predicted, k=5):\n",
    "    \"\"\"Calculate Precision@K\"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    \n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    \n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "    \n",
    "    return num_hits / min(len(predicted), k)\n",
    "\n",
    "def recall_at_k(actual, predicted, k=5):\n",
    "    \"\"\"Calculate Recall@K\"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    \n",
    "    num_hits = 0.0\n",
    "    for p in predicted:\n",
    "        if p in actual:\n",
    "            num_hits += 1.0\n",
    "    \n",
    "    return num_hits / len(actual) if len(actual) > 0 else 0.0\n",
    "\n",
    "def ndcg_at_k(actual, predicted, k=5):\n",
    "    \"\"\"Calculate Normalized Discounted Cumulative Gain@K\"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    \n",
    "    dcg = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual:\n",
    "            dcg += 1.0 / np.log2(i + 2)\n",
    "    \n",
    "    idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(actual), k))])\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "print(\"Evaluation metrics defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f02f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test set: For each user, hold out their most recent interactions\n",
    "test_users = []\n",
    "test_actual = []\n",
    "train_interactions = interaction_matrix.copy()\n",
    "\n",
    "print(\"Creating train/test split...\")\n",
    "print(f\"Original interaction matrix shape: {interaction_matrix.shape}\")\n",
    "\n",
    "# For each user, identify their top interacted packages as \"ground truth\"\n",
    "for user_id in interaction_matrix.index[:100]:  # Evaluate on first 100 users\n",
    "    user_interactions = interaction_matrix.loc[user_id]\n",
    "    interacted = user_interactions[user_interactions > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if len(interacted) >= 3:  # Only evaluate users with at least 3 interactions\n",
    "        # Hold out top 20% of interactions for testing\n",
    "        n_test = max(1, int(len(interacted) * 0.2))\n",
    "        test_packages = interacted.index[:n_test].tolist()\n",
    "        \n",
    "        test_users.append(user_id)\n",
    "        test_actual.append(test_packages)\n",
    "        \n",
    "        # Remove test interactions from training matrix\n",
    "        train_interactions.loc[user_id, test_packages] = 0\n",
    "\n",
    "print(f\"\\nTest set created!\")\n",
    "print(f\"Number of test users: {len(test_users)}\")\n",
    "print(f\"Average test packages per user: {np.mean([len(x) for x in test_actual]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf59334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain models on training data only\n",
    "print(\"Retraining models on training data...\")\n",
    "\n",
    "# Retrain SVD on training data\n",
    "svd_train = TruncatedSVD(n_components=20, random_state=42)\n",
    "user_factors_train = svd_train.fit_transform(train_interactions)\n",
    "package_factors_train = svd_train.components_.T\n",
    "\n",
    "predicted_scores_train = np.dot(user_factors_train, package_factors_train.T)\n",
    "predicted_scores_train_df = pd.DataFrame(\n",
    "    predicted_scores_train,\n",
    "    index=train_interactions.index,\n",
    "    columns=train_interactions.columns\n",
    ")\n",
    "\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Explained variance: {svd_train.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Collaborative Filtering Model\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATING COLLABORATIVE FILTERING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "collab_metrics = {\n",
    "    'precision@5': [],\n",
    "    'precision@10': [],\n",
    "    'recall@5': [],\n",
    "    'recall@10': [],\n",
    "    'ndcg@5': [],\n",
    "    'ndcg@10': []\n",
    "}\n",
    "\n",
    "for user_id, actual_packages in zip(test_users, test_actual):\n",
    "    # Get predictions from collaborative filtering\n",
    "    user_pred = predicted_scores_train_df.loc[user_id]\n",
    "    \n",
    "    # Remove packages user already interacted with in training\n",
    "    train_interacted = train_interactions.loc[user_id]\n",
    "    train_interacted_packages = train_interacted[train_interacted > 0].index.tolist()\n",
    "    \n",
    "    # Get top predictions\n",
    "    available_pred = user_pred[~user_pred.index.isin(train_interacted_packages)]\n",
    "    top_10 = available_pred.sort_values(ascending=False).head(10).index.tolist()\n",
    "    top_5 = top_10[:5]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    collab_metrics['precision@5'].append(precision_at_k(actual_packages, top_5, k=5))\n",
    "    collab_metrics['precision@10'].append(precision_at_k(actual_packages, top_10, k=10))\n",
    "    collab_metrics['recall@5'].append(recall_at_k(actual_packages, top_5, k=5))\n",
    "    collab_metrics['recall@10'].append(recall_at_k(actual_packages, top_10, k=10))\n",
    "    collab_metrics['ndcg@5'].append(ndcg_at_k(actual_packages, top_5, k=5))\n",
    "    collab_metrics['ndcg@10'].append(ndcg_at_k(actual_packages, top_10, k=10))\n",
    "\n",
    "# Print results\n",
    "print(\"\\nCollaborative Filtering Results:\")\n",
    "print(\"-\"*80)\n",
    "for metric, values in collab_metrics.items():\n",
    "    print(f\"{metric:20s}: {np.mean(values):.4f} (±{np.std(values):.4f})\")\n",
    "\n",
    "print(f\"\\nEvaluated on {len(test_users)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e9afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Hybrid Model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING HYBRID MODEL (60% Collaborative + 40% Content)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "hybrid_metrics = {\n",
    "    'precision@5': [],\n",
    "    'precision@10': [],\n",
    "    'recall@5': [],\n",
    "    'recall@10': [],\n",
    "    'ndcg@5': [],\n",
    "    'ndcg@10': []\n",
    "}\n",
    "\n",
    "for user_id, actual_packages in zip(test_users, test_actual):\n",
    "    # Get collaborative scores\n",
    "    collab_scores = predicted_scores_train_df.loc[user_id]\n",
    "    \n",
    "    # Get content-based scores from last interaction in training\n",
    "    train_interacted = train_interactions.loc[user_id]\n",
    "    train_interacted_packages = train_interacted[train_interacted > 0]\n",
    "    \n",
    "    if len(train_interacted_packages) > 0:\n",
    "        last_package = train_interacted_packages.idxmax()\n",
    "        content_scores = content_similarity_df[last_package]\n",
    "    else:\n",
    "        content_scores = pd.Series(0, index=collab_scores.index)\n",
    "    \n",
    "    # Normalize and combine\n",
    "    collab_norm = (collab_scores - collab_scores.min()) / (collab_scores.max() - collab_scores.min() + 1e-10)\n",
    "    content_norm = (content_scores - content_scores.min()) / (content_scores.max() - content_scores.min() + 1e-10)\n",
    "    \n",
    "    hybrid_scores = 0.6 * collab_norm + 0.4 * content_norm\n",
    "    \n",
    "    # Get top predictions (excluding training interactions)\n",
    "    available_pred = hybrid_scores[~hybrid_scores.index.isin(train_interacted_packages.index)]\n",
    "    top_10 = available_pred.sort_values(ascending=False).head(10).index.tolist()\n",
    "    top_5 = top_10[:5]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    hybrid_metrics['precision@5'].append(precision_at_k(actual_packages, top_5, k=5))\n",
    "    hybrid_metrics['precision@10'].append(precision_at_k(actual_packages, top_10, k=10))\n",
    "    hybrid_metrics['recall@5'].append(recall_at_k(actual_packages, top_5, k=5))\n",
    "    hybrid_metrics['recall@10'].append(recall_at_k(actual_packages, top_10, k=10))\n",
    "    hybrid_metrics['ndcg@5'].append(ndcg_at_k(actual_packages, top_5, k=5))\n",
    "    hybrid_metrics['ndcg@10'].append(ndcg_at_k(actual_packages, top_10, k=10))\n",
    "\n",
    "# Print results\n",
    "print(\"\\nHybrid Model Results:\")\n",
    "print(\"-\"*80)\n",
    "for metric, values in hybrid_metrics.items():\n",
    "    print(f\"{metric:20s}: {np.mean(values):.4f} (±{np.std(values):.4f})\")\n",
    "\n",
    "print(f\"\\nEvaluated on {len(test_users)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca7a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL ACCURACY COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Collaborative Filtering': [\n",
    "        np.mean(collab_metrics['precision@5']),\n",
    "        np.mean(collab_metrics['precision@10']),\n",
    "        np.mean(collab_metrics['recall@5']),\n",
    "        np.mean(collab_metrics['recall@10']),\n",
    "        np.mean(collab_metrics['ndcg@5']),\n",
    "        np.mean(collab_metrics['ndcg@10'])\n",
    "    ],\n",
    "    'Hybrid Model': [\n",
    "        np.mean(hybrid_metrics['precision@5']),\n",
    "        np.mean(hybrid_metrics['precision@10']),\n",
    "        np.mean(hybrid_metrics['recall@5']),\n",
    "        np.mean(hybrid_metrics['recall@10']),\n",
    "        np.mean(hybrid_metrics['ndcg@5']),\n",
    "        np.mean(hybrid_metrics['ndcg@10'])\n",
    "    ]\n",
    "}, index=['Precision@5', 'Precision@10', 'Recall@5', 'Recall@10', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "print(\"\\n\", comparison_df)\n",
    "\n",
    "# Calculate improvement\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Hybrid Model Improvement over Collaborative Filtering:\")\n",
    "print(\"=\"*80)\n",
    "for metric in comparison_df.index:\n",
    "    collab_val = comparison_df.loc[metric, 'Collaborative Filtering']\n",
    "    hybrid_val = comparison_df.loc[metric, 'Hybrid Model']\n",
    "    improvement = ((hybrid_val - collab_val) / collab_val * 100) if collab_val > 0 else 0\n",
    "    print(f\"{metric:20s}: {improvement:+.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Overall Model Accuracy: {np.mean(hybrid_metrics['ndcg@10'])*100:.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
